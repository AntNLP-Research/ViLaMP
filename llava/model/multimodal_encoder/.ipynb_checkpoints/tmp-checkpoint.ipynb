{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a4e62-8c1c-4bf8-a793-e5db8e7b8e30",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional, Tuple, Union, Dict\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "if is_flash_attn_2_available():\n",
    "    from flash_attn import flash_attn_varlen_func, flash_attn_func\n",
    "\n",
    "    from transformers.modeling_flash_attention_utils import _flash_attention_forward\n",
    "else:\n",
    "    flash_attn_varlen_func = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae716ce0-95e6-4d9d-964c-ba2dae39a1c4",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 768\n",
    "        self.num_attention_heads = 12\n",
    "        self.attention_dropout = 0.0\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9cfb8c-7940-4c68-a1e6-d764e83d6e4d",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "class VisionFlashAttention2(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, dtype=torch.bfloat16)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, dtype=torch.bfloat16)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, dtype=torch.bfloat16)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, dtype=torch.bfloat16)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = False\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n",
    "        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n",
    "        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        attn_output = flash_attn_func(query_states, key_states, value_states)\n",
    "        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim).contiguous()\n",
    "\n",
    "        # cu_seqlens = [0] + [q_len] * batch_size\n",
    "        # cu_seqlens = torch.Tensor(cu_seqlens).cumsum(dim=0, dtype=torch.int32)\n",
    "        # max_seqlen = max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n",
    "\n",
    "        # attn_output = flash_attn_varlen_func(query_states, key_states, value_states, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen).reshape(\n",
    "        #     batch_size, q_len, self.embed_dim\n",
    "        # )\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb71b6-6c81-4129-80cc-975c6699960f",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "class SigLipAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    # Copied from transformers.models.clip.modeling_clip.CLIPAttention.__init__\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\" f\" {self.num_heads}).\")\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, dtype=torch.bfloat16)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, dtype=torch.bfloat16)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, dtype=torch.bfloat16)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, dtype=torch.bfloat16)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        batch_size, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        k_v_seq_len = key_states.shape[-2]\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n",
    "\n",
    "        if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n",
    "            raise ValueError(f\"Attention weights should be of size {(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is\" f\" {attn_weights.size()}\")\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):\n",
    "                raise ValueError(f\"Attention mask should be of size {(batch_size, 1, q_len, k_v_seq_len)}, but is {attention_mask.size()}\")\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\" f\" {attn_output.size()}\")\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df257c4-5023-4418-aa75-b278065ffe41",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "flash_attn = VisionFlashAttention2(config).cuda()\n",
    "siglip_attn = SigLipAttention(config).cuda()\n",
    "flash_attn.q_proj = siglip_attn.q_proj\n",
    "flash_attn.k_proj = siglip_attn.k_proj\n",
    "flash_attn.v_proj = siglip_attn.v_proj\n",
    "flash_attn.out_proj = siglip_attn.out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332bc1eb-3092-49a9-841b-c5dab42a741b",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "x = torch.rand((10, 729, 768), dtype=torch.bfloat16).cuda()\n",
    "flash_attn_output, _ = flash_attn(x)\n",
    "siglip_attn_output, _ = siglip_attn(x)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
